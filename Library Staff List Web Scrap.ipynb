{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_genre_subject(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Click on the \"Full details\" button\n",
    "    driver.find_element(By.XPATH, \"//a[@data-key='full-details-link']\").click()\n",
    "\n",
    "    # Wait for the overlay to appear (you may need to adjust the wait time)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Get the HTML content after the overlay is loaded\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Now you can use BeautifulSoup to extract genre and subject values\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the genre label and ul element\n",
    "    genre_label = soup.select_one('.cp-bib-field-label:-soup-contains(\"Genre\")')\n",
    "\n",
    "    # Creating the genre_label\n",
    "    if genre_label:\n",
    "        genre_ul = genre_label.find_next('ul', class_='values-list')\n",
    "        # Extract the text content from each span within the li elements\n",
    "        genre_values = [li.select_one('span.formatted-value').text for li in genre_ul.find_all('li')]\n",
    "        # Remove dots and extra spaces from genre values\n",
    "        genre_values = ', '.join([genre.replace(\".\", \"\").strip() for genre in genre_values])  \n",
    "    else:\n",
    "        genre_values = None\n",
    "\n",
    "    # Similarly, extract subject values\n",
    "    subject_label = soup.select_one('.cp-bib-field-label:-soup-contains(\"Subject\")')\n",
    "\n",
    "    # Creating the subject_label\n",
    "    if subject_label:\n",
    "        subject_ul = subject_label.find_next('ul', class_='values-list')\n",
    "        # Extract the text content from each span within the li elements\n",
    "        subject_values = [li.select_one('span.formatted-value').text for li in subject_ul.find_all('li')]\n",
    "        # Remove dots and extra spaces from subject values\n",
    "        subject_values = ', '.join([subject.replace(\".\", \"\").strip() for subject in subject_values])\n",
    "    else:\n",
    "        subject_values = None\n",
    "\n",
    "    # Don't forget to close the Selenium WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return genre_values, subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing \"get_genre_subject\" function \n",
    "\n",
    "url = \"https://kdl.bibliocommons.com/v2/record/S174C631702\"\n",
    "\n",
    "genre_values, subject_values = get_genre_subject(url) # one problem with this approach of inserting commas as separators is that some subjects/genres already have commas in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking in a KDL link to get basic information about a single book\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def book_info(url):\n",
    "\n",
    "    html = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    title = soup.find(\"h1\" ,class_='cp-heading heading-modest title heading--linked') #Find_all does not have a text attribute so would need to use find\n",
    "    title = title.text\n",
    "    t_length = int(len(title)/2)\n",
    "\n",
    "    # Description - need a try block here for titles that don't have descriptions listed \n",
    "    try:\n",
    "        description = soup.find(\"div\", class_=\"cp-bib-description\").text\n",
    "    except AttributeError:\n",
    "        description = \"No description found in Bibliocommons\" \n",
    "\n",
    "    # Availability\n",
    "    sid = soup.find(id='content')\n",
    "    status = sid.find(\"span\", class_ = \"cp-screen-reader-message cp-format-chooser-sr-message\")\n",
    "    status = status.text.split(\",\")[4].replace(\".\", \"\").strip()\n",
    "\n",
    "    # Rating \n",
    "    u_rating = soup.find(class_='rating-info')\n",
    "    rating = u_rating.text.split(\"(\")[0][12:]\n",
    "\n",
    "    # Item Type\n",
    "    sid = soup.find(id='content')\n",
    "    item_type = sid.find(\"span\", class_ = \"cp-screen-reader-message cp-format-chooser-sr-message\")\n",
    "    item_type = item_type.text.split(\",\")[1].strip()\n",
    "\n",
    "    # Genre & Subject\n",
    "    genre_values, subject_values = get_genre_subject(url)\n",
    "\n",
    "    # Author\n",
    "    author = soup.find(\"div\", class_='cp-author-link')\n",
    "    author = author.text\n",
    "    a_length = int(len(author)/2)  \n",
    "\n",
    "    #print(\"Title: \" + title[t_length:] + \"\\n\" + \"author\" + author[length:] + \"\\n\" + \"Item Type: \" + item_type + \"\\n\" + \"Rating: \" + rating + \"\\n\" \"Status: \" + status + \"\\n\" \"Description: \" + description + \"\\n\")\n",
    " \n",
    "    new_data = {\n",
    "    \"Title\": title[t_length:],\n",
    "    \"Author\": author[a_length:],\n",
    "    \"Item Type\": item_type,\n",
    "    \"Rating\": rating,\n",
    "    \"Status\": status,\n",
    "    \"Description\": description,\n",
    "    'Genre': genre_values,\n",
    "    'Subject': subject_values,\n",
    "    'Link': url\n",
    "    } \n",
    "\n",
    "    new_data = pd.DataFrame(new_data, index=[0])\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out \"book_info\" function \n",
    "url = \"https://kdl.bibliocommons.com/v2/record/S174C940822\"\n",
    "data = book_info(url)\n",
    "\n",
    "# Saving the data gathered from \"book_info\" function and saving it as a df\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Item Type\", \"Rating\", \"Status\", \"Description\", \"Genre\", \"Subject\", \"Link\"], index=None) # need to initialize df\n",
    "\n",
    "df = pd.concat([df, data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting getting # of copies and avaliability  \n",
    "# All of the circ info is there on the webpage but from some reason I can get that HTML code when I scrap it.... \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming html_content contains your HTML code\n",
    "html_content = \"\"\"\n",
    "<div class=\"panel-section circulation-section\"><div class=\"cp-circulation-info\"><span class=\"availability-status\"><span class=\"cp-availability-status available\" data-key=\"availability-status-available\">Available but not holdable</span></span><div class=\"circulation-details\"><div><span aria-hidden=\"true\" class=\"total-copies-count\"><span class=\"circulation-count\">41</span> copies</span><span class=\"cp-screen-reader-message\">41 copies</span></div><div><span aria-hidden=\"true\" class=\"available-count\"><span class=\"circulation-count\">3</span> available</span><span class=\"cp-screen-reader-message\">3 available</span></div><div><span aria-hidden=\"true\" class=\"on-hold-count\"><span class=\"circulation-count\">108</span> on hold</span><span class=\"cp-screen-reader-message\">108 on hold</span></div></div></div></div>\n",
    "\"\"\"\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the span with class \"availability-status\" and get its text\n",
    "circs = soup.find_all('div', class_='circulation-details')\n",
    "\n",
    "for circ in circs:\n",
    "    print(circ.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Getting the number of pages that the staff list has so that I can then itterate by that number\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import math\n",
    "import time\n",
    "\n",
    "url = \"https://kdl.bibliocommons.com//list/share/1709636399_kdl_adults/2480807429_literary_easter_eggs_in_tom_lake_by_ann_patchett\"\n",
    "html = requests.get(url)\n",
    "s = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "total_books = s.find('span', class_='item_count')  # Accounting for webpages that have multiple pages \n",
    "if total_books is None:\n",
    "    total_books = s.find('span', class_='item_count_label')\n",
    "\n",
    "match = re.search(r'\\d+', total_books.text)\n",
    "if match:\n",
    "    totbok = int(match.group())\n",
    "    \n",
    "    numpages = math.ceil(totbok/25) # Using math.ceil to round up answer to nearest whole number\n",
    "    \n",
    "    print(numpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the item ids of one staff list page scraped \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "item_id_list = []\n",
    "url = \"https://kdl.bibliocommons.com/list/share/2069242809_cekovach1/2480796599_teen_must_reads\"\n",
    "html = requests.get(url)\n",
    "\n",
    "s = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "#results = s.find(id='main') # in case we need it \n",
    "book_divs = s.find_all('div', class_='list_item_title')\n",
    "\n",
    "for book_div in book_divs:\n",
    "    link = book_div.find('a')\n",
    "\n",
    "    if link:\n",
    "        item_id = link['href'].split(\"/\")[3]\n",
    "\n",
    "        if item_id.endswith(\"174\"):\n",
    "            item_id = int(item_id[:-3])\n",
    "            item_id_list.append(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over the URL to get every page of books scraped \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "item_id_list = []\n",
    "url = \"https://kdl.bibliocommons.com/list/share/2069242809_cekovach1/2480796599_teen_must_reads\"\n",
    "\n",
    "for page in range(1, numpages + 1):  # odd way of writing this because range(4) would output as 0,1,2,3. So I have to set a starting point of 1 and add 1 to numpages at the end\n",
    "    full_url = url + f\"?page={page}\"\n",
    "    html = requests.get(full_url)\n",
    "\n",
    "    sleep_time = random.randint(1,10)\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    s = BeautifulSoup(html.content, 'html.parser')\n",
    "    #results = s.find(id='main') # in case we need it \n",
    "    book_divs = s.find_all('div', class_='list_item_title')\n",
    "\n",
    "    for book_div in book_divs:\n",
    "        link = book_div.find('a')\n",
    "        \n",
    "        if link:\n",
    "            item_id = link['href'].split(\"/\")[3]\n",
    "\n",
    "            if item_id.endswith(\"174\"):  # inadvertently filters out all non-books in list\n",
    "                item_id = int(item_id[:-3])\n",
    "                item_id_list.append(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the entire list of book id numbers to get their information \n",
    "\n",
    "#with the current settings it took 11 mins 30 seconds to run, also only produces 72 of 78 items because we're excluding non books atm *shrug*\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Item Type\", \"Rating\", \"Status\", \"Description\"], index=None) # Initialzing our data to store all output from book_info function \n",
    "intervals = 0 \n",
    "\n",
    "newlist = item_id_list[:5]\n",
    "\n",
    "for id in newlist:\n",
    "    url = f\"https://kdl.bibliocommons.com/v2/record/S174C{id}\"\n",
    "\n",
    "    sleep_time = random.randint(1,10)\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    info = book_info(url)\n",
    "    df = df.append(info, ignore_index=True)\n",
    "    \n",
    "    intervals += 1\n",
    "    print(intervals, \"out of \", totbok, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"get_books_from_staff_list\" function takes a staff list as input and returns information about all the books in that staff list\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Run book_info & get_genre_subject function first!\n",
    "\n",
    "def get_books_from_staff_list(staff_pick_url):\n",
    "\n",
    "    html = requests.get(staff_pick_url)\n",
    "    s = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    sleep_time = random.randint(1,15) # Sleep time adjustments \n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    total_books = s.find('span', class_='item_count')  # Accounting for webpages that have multiple pages \n",
    "    if total_books is None:\n",
    "        total_books = s.find('span', class_='item_count_label')\n",
    "\n",
    "    match = re.search(r'\\d+', total_books.text) # keeps any digits in the string\n",
    "    if match:\n",
    "        totbok = int(match.group())\n",
    "        \n",
    "        numpages = math.ceil(totbok/25) # Using math.ceil to round up answer to nearest whole number\n",
    "\n",
    "    time.sleep(2) # breaking up the inital page requests - Sleep time adjustments \n",
    "\n",
    "    item_id_list = [] # initalizing our list to store item ids\n",
    "\n",
    "    for page in range(1, numpages + 1):  # odd way of writing this because range(4) would output as 0,1,2,3. So I have to set a starting point of 1 and add 1 to numpages at the end\n",
    "        full_url = staff_pick_url + f\"?page={page}\"\n",
    "        html = requests.get(full_url)\n",
    "\n",
    "        sleep_time = random.randint(1,15) # Sleep time adjustments \n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        s = BeautifulSoup(html.content, 'html.parser')\n",
    "        book_divs = s.find_all('div', class_='list_item_title')\n",
    "\n",
    "        for book_div in book_divs:\n",
    "            link = book_div.find('a')\n",
    "            \n",
    "            if link:\n",
    "                item_id = link['href'].split(\"/\")[3]\n",
    "\n",
    "                if item_id.endswith(\"174\"):  # inadvertently filters out all non-books in list\n",
    "                    item_id = int(item_id[:-3])\n",
    "                    item_id_list.append(item_id)\n",
    "\n",
    "    name_of_staff_list = s.find('h1', class_='list_title') #Gathers the name of the staff list to be printed out \n",
    "    name_of_staff_list = name_of_staff_list.text.strip()\n",
    "\n",
    "    staff_list_books = pd.DataFrame(columns=[\"Title\", \"Author\", \"Item Type\", \"Rating\", \"Status\", \"Description\", \"Genre\", \"Subject\", \"Link\"], index=None) # Initialzing our data to store all output from book_info function \n",
    "    intervals = 0 \n",
    "\n",
    "    for id in item_id_list:\n",
    "        url = f\"https://kdl.bibliocommons.com/v2/record/S174C{id}\"\n",
    "\n",
    "        sleep_time = random.randint(1,15)  # Sleep time adjustments\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        data = book_info(url)\n",
    "        #staff_list_books = staff_list_books.append(data, ignore_index=True)\n",
    "\n",
    "        staff_list_books = pd.concat([staff_list_books, data], ignore_index=True)\n",
    "\n",
    "        intervals += 1\n",
    "        print(f\"Scraped item {intervals} out of {totbok} from \\\"{name_of_staff_list}\\\" Staff List \\n\")\n",
    "\n",
    "    return staff_list_books, item_id_list, totbok, numpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running \"get_books_from_staff_list\" Function \n",
    "# Takes 22 minutes to run on Teen Must Reads list\n",
    "\n",
    "staff_pick_url = \"https://kdl.bibliocommons.com/list/share/1709638099_kdl_staffpicks/2469845619_anticipated_new_non-fiction\" \n",
    "\n",
    "staff_list_books, item_id_list, totbok, numpages = get_books_from_staff_list(staff_pick_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save a df to an excel file if desired\n",
    "file_path = r\"C:\\Users\\Ryan\\Desktop\\book_list2.xlsx\"\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "#print(f\"DataFrame saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering a list of all of the staff lists from a URL  \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def staff_list_accumulation(list_of_staff_lists):\n",
    "\n",
    "    html = requests.get(list_of_staff_lists)\n",
    "    s = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "    # Find all date elements\n",
    "    date_boxes = s.find_all('div', class_='dataPair clearfix small list_created_date')\n",
    "\n",
    "    # Find all titles, categories, and descriptions\n",
    "    titles = s.find_all('span', class_='title')\n",
    "    categories = s.find_all('div', class_='list_type small')\n",
    "    descriptions = s.find_all('div', class_='description')\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    sl_df = pd.DataFrame(columns=['SL_Title', 'Category', 'SL_Description', 'SL_Created', 'SL_Link'])\n",
    "\n",
    "    # Iterate through both the date elements and titles/categories/descriptions\n",
    "    for date_box, title, category, description in zip(date_boxes, titles, categories, descriptions):\n",
    "        # Extract the date\n",
    "        date_u = date_box.find('span', class_='value')\n",
    "        date = datetime.strptime(date_u.text.strip(), '%b %d, %Y') if date_u else None\n",
    "\n",
    "        # Extract book list link \n",
    "        link = title.find('a')\n",
    "        if link:\n",
    "            link = (link.get('href'))\n",
    "            init_url = \"https://kdl.bibliocommons.com/\"\n",
    "            full_link = init_url + link\n",
    "\n",
    "        # Extract title, category, and description\n",
    "        title_text = title.text.strip() \n",
    "        category_text = category.text.strip() \n",
    "        description_text = description.text.strip() #if description else None\n",
    "\n",
    "        # Append to the DataFrame\n",
    "        new_row = {'SL_Title': title_text, 'Category': category_text, 'SL_Description': description_text, 'SL_Created': date, 'SL_Link': full_link}\n",
    "        sl_df = pd.concat([sl_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return sl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running \"staff_list_accumulation\" function by itself to get information about many staff lists from a KDL URL\n",
    "\n",
    "list_of_staff_lists = \"https://kdl.bibliocommons.com/search?creator_library=174&creator_type=STAFF&display_quantity=2&page=1&q=ignored&search_category=alllists&supress=true&t=alllists&title=All+staff+lists\"\n",
    "\n",
    "sl_df = staff_list_accumulation(list_of_staff_lists)\n",
    "\n",
    "#sl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        ###################################################### THE MONSTER ######################################################\n",
    "\n",
    "# Code to look at a list of staff lists, itterate through each list, then scrape the list of books of the list and append to a monster_books, combines everything done above\n",
    "\n",
    "list_of_staff_lists = \"https://kdl.bibliocommons.com/search?creator_library=174&creator_type=STAFF&display_quantity=1&page=6&q=ignored&search_category=alllists&supress=true&t=alllists&title=All+staff+lists\"\n",
    "\n",
    "sl_df = staff_list_accumulation(list_of_staff_lists)\n",
    "\n",
    "staff_list_links = sl_df['SL_Link'].tolist()\n",
    "staff_list_titles = sl_df['SL_Title'].tolist()\n",
    "\n",
    "# Assuming you have an existing DataFrame called monster_books\n",
    "monster_books = pd.DataFrame()\n",
    "\n",
    "# Iterate through staff_list_links\n",
    "for i, staff_pick_url in enumerate(staff_list_links):\n",
    "    # Call the function and get the results\n",
    "    staff_list_books, item_id_list, totbok, numpages = get_books_from_staff_list(staff_pick_url)\n",
    "\n",
    "    # Add a 'Staff List Name' column to the staff_list_books DataFrame\n",
    "    staff_list_books['Staff List Name'] = staff_list_titles[i]\n",
    "\n",
    "    # Append the staff_list_books DataFrame to the existing DataFrame\n",
    "    monster_books = pd.concat([monster_books, staff_list_books], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the books and staff lists DataFrames on the together\n",
    "\n",
    "all_results_df = pd.merge(monster_books, sl_df, how='inner', left_on='Staff List Name', right_on='SL_Title')\n",
    "\n",
    "# Drop the duplicate column 'title' (if needed)\n",
    "all_results_df.drop(columns=['SL_Title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save a df to an excel file if desired\n",
    "file_path = r\"C:\\Users\\Ryan\\Coding Projects\\Web Scraping\\Saved Data Files\\book_list_monster.xlsx\"\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "all_results_df.to_excel(file_path, index=False)\n",
    "\n",
    "#print(f\"DataFrame saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the number of pages in a staff list section - \n",
    "# not really relavent because you can put 100 staff lists on a single page in the URL and I won't be doing more than that unless I go psycho\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import math\n",
    "import time\n",
    "\n",
    "url = \"https://kdl.bibliocommons.com/search?creator_library=174&creator_type=STAFF&display_quantity=100&page=2&q=ignored&search_category=alllists&supress=true&t=alllists&title=All+staff+lists\"\n",
    "html = requests.get(url)\n",
    "s = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "total_list = s.find('div', class_='utility_bar_paginate')  # Accounting for webpages that have multiple pages \n",
    "digits = re.findall(r'\\d+', total_list.text) # Using regex to find all digits that are put into a list \n",
    "total_staff_lists = int(digits[2])\n",
    "\n",
    "staff_list_pages = math.ceil(total_staff_lists/25)\n",
    "staff_list_pages\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
